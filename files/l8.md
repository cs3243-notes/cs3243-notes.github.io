---
title: "Chapter 8"
permalink: /files/l8/
author_profile: false
---

# 8. Games: Adversarial Search

Until now, we considered the problems that are Single Agent, Fully Observable, Deterministic and Goal-oriented. There are many other problems involving multiple agents, sometimes adversarial to one another. Let us discuss the  problems with multiple agents which are adversarial to each other.
 
 Let us consider the scenario of a two player game. In this game, there are $2$ bags and there are $2$ balls in each bag with a number on it. There are 2 players, one aiming to maximize the score, let  us call him/her as MAX player, and the other one is aiming to minimize the score, again let us call him/her as  MIN player. The MAX player has to chose a bag, while the MIN player has to  choose one ball out of the $2$ balls in the chosen bag. The score is determined by the number on the ball chosen. 

 Let the 2 bags be $B^1$ and $B^2$ and the 2 balls in bag $B^1$ be $B_1^1$ and $B_1^2$, and the 2 balls in bag $B^2$ be $B_2^1$ and $B_2^2$.  The Figure 1 shows the possible outcomes of the game. The red terminal in the Figure 1 represents the balls, and the number on them.

| ![Figure 1](/images/markdown-images/L8/maxmin_small1.pdf) |
|:--:|
| *Figure 1: Example to Max-Min 2 player game.* |

As per Figure 1, $s_0$, the root is the start state of this game, and $s_3, s_4, s_5, s_6$ as indicated by the red nodes are the terminal states where the score of the game is calculated. In addition, we have the 2 parameters depth $d$ and branching factor $b$.

Let utility function be $Utility(s)$ where s is a terminal state, i.e., for example, $Utility(s_3)=2$. The purpose of MAX player is to maximise the utility, while the purpose of MIN player is to minimize the utility. 

An intuitive strategy that both players can adopt is to assume that the opponent would play optimally. That is, MAX player would always choose state with the maximum utility, and the MIN player would always choose the minimum utility. Let our transition model function be $RESULT(s,a)$. $RESULT(s,a)$ considers a state $s$ and an action $a$ taken as input to return the next state.

We can derive an algorithm, $MINIMAX(s)$ as follows:

$$
MINIMAX(s) = \left\{
\begin{array}{ll}
Utility(s) & if \: s \: is \: terminal \: state\\
\max\limits_{a \in A} MINIMAX(RESULT(s, a)) & if \: MAX \: player \\
\min\limits_{a \in A} MINIMAX(RESULT(s, a)) & if \: MIN \: player \\
\end{array}
\right.
$$

We will now trace this $MINIMAX$ algorithm on the tree in Figure 2 and  3. This will give us the tree below, where nodes are circled when they are explored.

| ![Figure 2](/images/markdown-images/L8/maxmin_small2.pdf) |
|:--:|
| *Figure 2: Example to Max-Min 2 player game; Nodes $s_3$ and $s_4$ explored.* |

- Max player will first explore the $s_3$, and $Utility(s_3) =2$, so at this point, we know that $Utility(s_1) \leq 2$.

- As, Max player does not know the  $Utility(s_1)$, it will explore $s_4$, as  $Utility(s_4) = 7$, as $Utility(s_1)$ will be decided by Min player, Max player know that the $Utility(s_1)=2$. Refer to Figure 2.

- As shown in Figure 3, the next Max player will check for $s_5$. $Utility(s_5) =1$, so at this point, we know that $Utility(s_2) \leq 1$. From the prospective of Max player, terminal $s_6$ does not exists, as whatever be the Utility of $s_6$, Max player knows that the  $Utility(s_2) \leq 1$, as $Utility(s_1)=2$, he should choose $s_1$ over $s_2$.

| ![Figure 3](/images/markdown-images/L8/maxmin_small3.pdf) |
|:--:|
| *Figure 3: Example to Max-Min 2 player game; Nodes $s_3$, $s_4$, $s_5$  explored.* |

Hence, Max player should chose $B_1$ in order to win the game.

## 8.1. Minimax Algorithm

As discussed in two agent problem, the game can easily be solved by a recursive computation of the minimax values each node in the decision tree, working from the leaves to the root as the recursion unwinds. The algorithm is as follows assuming the MAX player starts first.

**Algorithm 1** MinimaxDecision($S$)
1.  $bestAction \leftarrow null$
2.  $max \leftarrow -\infty$
3.  **for** all action $a$ in ACTIONS($S$) **do**
4.  $~~~~~$ $util \leftarrow$ MinValue(RESULT($S$,$a$))
5.  $~~~~~$ **if** $util > max$ **then**
6.  $~~~~~~~~~~$ $bestAction \leftarrow a$
7.  $~~~~~~~~~~$ $max \leftarrow util$
8.  **return** $bestAction$
  
**Algorithm 2** MaxValue($S$)
1.  **if** TERMINAL($S$)=True **then return** UTILITY($S$)
2.  $v \leftarrow -\infty$
3.  **for** all action $a$ in ACTIONS($S$) **do**
4.  $~~~~~$ $v \leftarrow$ max($v$, MinValue(RESULT($S$,$a$)))
5.  **return** $v$
  
**Algorithm 3** MinValue($S$)
1.  **if** TERMINAL($S$)=True **then return** UTILITY($S$)
2.  $v \leftarrow \infty$
3.  **for** all action $a$ in ACTIONS($S$) **do**
4.  $~~~~~$ $v \leftarrow$ min($v$, MaxValue(RESULT($S$,$a$)))
5.  **return** $v$

The *MinimaxDecision* function can be modified to produce a set of optimal rational strategy for the game. A strategy is defined a a mapping of states in the game to an action or a series of actions. In other words, Strategy: $States \mapsto Actions$

### 8.1.1. Evaluation Time/Space Complexity of Minimax Algorithm

The *Minimax* algorithm implements a complete depth-first exploration of the game tree. The functions *MAXVALUE* and *MINVALUE* traverses the entire decision tree, down to each of the leaves, to evaluate the backed-up value of a state represented by a branch node by backward induction. Assuming each game state represented by a node has a branching factor $b$ and the game tree has a maximum depth of $m$, the number of terminal states to evaluate would be $b^m$, which means the *Minimax* algorithm has a time complexity of $O(b^m)$. This is inefficient in practice. The space complexity of the *Minimax* algorithm is $O(bm)$ for if it generates the entire game tree at once, and $O(m)$ if it generates the actions of the game tree one at a time.

## 8.2. Alpha-Beta Pruning

Next we move on to an alternative algorithm, ...

<Insert Algorithm 4>

<Insert Algorithm 5>

<Insert Algorithm 6>

### 8.2.1. An Example for Alpha-Beta Pruning

Let us consider the game tree ...

<Insert Figure 4>

<Insert Figure 5>

### 8.2.2. Evaluation of Time/Space Complexity of Alpha-Beta Pruning

One weakness of the Alpha-beta pruning algorithm ...

## 8.3. Heuristic Function for Imperfect Decision-Making

Because Alpha-beta pruning still has to explore the game tree until the terminal state, this makes its time complexity impractical in games with large depth such as chess. One strategy is to cut off the search earlier, and apply a heuristic evaluation function to each state at the cutoff, thus allowing for non-terminal nodes to become leaves of the smaller search tree.

A good heuristic evaluation function should have the following properties:

1. The evaluation function should evaluate terminal states in a similar preference order as the true utility function would on the terminal states.

1. Computation of the evaluation function should not take too long (ideally close to constant time complexity)

1. For non-terminal states, the evaluation function should choose states highly correlated with the chances of winning. The uncertainty is induced by computational limits. 

For example in chess, an evaluation function the expected probability of a win based on a weighted linear function of certain features of the game. For example, the evaluation function can be,
$$EVAL(S)=w_1 f_1(S)+w_2 f_2(S)+\cdots +w_n f_n(S)$$
where $w_i$ refers to the weights, while $f_i(S)$ refers to a computation of features in the game state, such as the number of a type of chess piece on the board. The evaluation function need not calculate the actual probability of winning, but the ordinal relation between states via the evaluation function should be similar to that of the expected probability of winning.

The Alpha-beta pruning algorithm could implement the cut-off and heuristic by modifying the terminal condition of the algorithm.

**Algorithm 7** MinimaxDecision($S$)
1.  $bestAction \leftarrow null$
2.  $max \leftarrow -\infty$
3.  **for** all action $a$ in ACTIONS$(S)$ **do**
4.  $~~~~~$ $util \leftarrow$ MinValue(RESULT($S$,$a$), $1$, $-\infty$, $\infty$)
5.  $~~~~~$ **if** $util > max$ **then**
6.  $~~~~~~~~~~$ $bestAction \leftarrow a$
7.  $~~~~~~~~~~$ $max \leftarrow util$
8.  **return** $bestAction$

**Algorithm 8** MinValue($S$, $depth$, $\alpha$, $\beta$)
1.  **if** CUTOFFTEST($S, depth$)=True **then return** EVAL($S$)
2.  $v \leftarrow \infty$
3.  **for** all action $a$ in ACTIONS($S$) **do**
4.  $~~~~~$ $v \leftarrow$ min($v$, MaxValue(RESULT($S$,$a$), $depth+1$, $\alpha$, $\beta$))
5.  $~~~~~$ **if** $v \leq \alpha$ **then return** $v$
6.  $~~~~~$ $\beta \leftarrow$ min($v,\beta$)
7.  **return** $v$

**Algorithm 9** MaxValue($S$, $depth$, $\alpha$, $\beta$)
1.  **if** CUTOFFTEST($S, depth$)=True **then return** EVAL($S$)
2.  $v \leftarrow -\infty$
3.  **for** all action $a$ in ACTIONS($S$) **do**
4.  $~~~~~$ $v \leftarrow$ max($v$, MinValue(RESULT($S$,$a$), $depth+1$, $\alpha$, $\beta$))
5.  $~~~~~$ **if** $v \geq \beta$ **then return** $v$ $~~~~~~~~~~$ $\text{// Pruning of branch}$
6.  $~~~~~$ $\alpha \leftarrow$ max($\alpha,v$)
7.  **return** $v$

One way to control the amount of search is to set a depth limit $d$ so that CUTOFFTEST($S, depth$) returns true when depth reaches limit $d$ or a terminal state is reached whichever is earlier. The depth limit can be determined by the deepest depth a computer can explore while just keeping to the time limit required to make a decision. With a cut-off, the time complexity improves to $O(b^{3d/4})$ on average where $d<m$.
